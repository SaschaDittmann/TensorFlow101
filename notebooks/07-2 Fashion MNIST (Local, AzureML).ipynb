{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST\n",
    "\n",
    "## Validate Azure ML SDK installation and get version number for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, Run, Workspace\n",
    "import azureml.core\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "Initialize a workspace object from persisted configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project directory\n",
    "Create a directory that will contain all the output from this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = '../projects/fashion_mnist_logtoazure'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Fashion MNIST and Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "data_folder = '../data/fashion_mnist'\n",
    "os.makedirs(data_folder, exist_ok = True)\n",
    "urllib.request.urlretrieve('https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/train-images-idx3-ubyte.gz', filename=os.path.join(data_folder, \"train-images.gz\"))\n",
    "urllib.request.urlretrieve('https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/train-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, \"train-labels.gz\"))\n",
    "urllib.request.urlretrieve('https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/t10k-images-idx3-ubyte.gz', filename=os.path.join(data_folder, \"test-images.gz\"))\n",
    "urllib.request.urlretrieve('https://github.com/zalandoresearch/fashion-mnist/raw/master/data/fashion/t10k-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, \"test-labels.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "\n",
    "def load_data(filename, label=False):\n",
    "    with gzip.open(filename) as gz:\n",
    "        struct.unpack('I', gz.read(4))\n",
    "        n_items = struct.unpack('>I', gz.read(4))\n",
    "        if not label:\n",
    "            n_rows = struct.unpack('>I', gz.read(4))[0]\n",
    "            n_cols = struct.unpack('>I', gz.read(4))[0]\n",
    "            res = np.frombuffer(gz.read(n_items[0] * n_rows * n_cols), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], n_rows * n_cols)\n",
    "        else:\n",
    "            res = np.frombuffer(gz.read(n_items[0]), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], 1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes - do not change unless the data changes\n",
    "num_classes = 10\n",
    "\n",
    "# sizes of batch and # of epochs of data\n",
    "batch_size = 128\n",
    "epochs = 24\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "x_train = load_data(os.path.join(data_folder, 'train-images.gz'), False)\n",
    "x_test = load_data(os.path.join(data_folder, 'test-images.gz'), False)\n",
    "y_train = load_data(os.path.join(data_folder, 'train-labels.gz'), True)\n",
    "y_test = load_data(os.path.join(data_folder, 'test-labels.gz'), True)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "#   Deal with format issues between different backends.  Some put the # of channels in the image before the width and height of image.\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "#   Type convert and scale the test and training data\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape (after reshape):', x_train.shape)\n",
    "print('x_test shape (after reshape):', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_index = 1\n",
    "plt.imshow(1-x_train[0][:, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Description \n",
    "label_dict = {\n",
    "    0: 'T-shirt/top',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before:\\n{}\".format(y_train[:4]))\n",
    "# convert class vectors to binary class matrices.  One-hot encoding\n",
    "#  3 => 0 0 0 1 0 0 0 0 0 0 and 1 => 0 1 0 0 0 0 0 0 0 0 \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(\"After:\\n{}\".format(y_train[:4]))  # verify one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Take a look at the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   define compile to minimize categorical loss, use ada delta optimized, and optimize to maximizing accuracy\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'fashion-mnist'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Define early stopping callback\n",
    "my_callbacks = [EarlyStopping(monitor='val_accuracy', patience=5, mode='max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with experiment.start_logging(outputs=None, snapshot_directory=project_folder) as run:\n",
    "    try:\n",
    "        run.tag(\"Description\",\"Locally trained Fashion MNIST model\")\n",
    "\n",
    "        #   Train the model and test/validate the mode with the test data after each cycle (epoch) through the training data\n",
    "        #   Return history of loss and accuracy for each epoch\n",
    "        hist = model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            callbacks=my_callbacks,\n",
    "            validation_data=(x_test, y_test))\n",
    "        run.log_list('Training Accuracy', hist.history['accuracy'])\n",
    "        run.log_list('Validation Accuracy', hist.history['val_accuracy'])\n",
    "\n",
    "        #   Evaluate the model with the test data to get the scores on \"real\" data.\n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "        run.log('loss', score[0])\n",
    "        run.log('accuracy', score[1])\n",
    "\n",
    "        #   Plot data to see relationships in training and validation data\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        epoch_list = list(range(1, len(hist.history['accuracy']) + 1))  # values for x axis [1, 2, ..., # of epochs]\n",
    "        plt.plot(epoch_list, hist.history['accuracy'], epoch_list, hist.history['val_accuracy'])\n",
    "\n",
    "        plt.legend(('Training Accuracy', 'Validation Accuracy'))\n",
    "        run.log_image(name='Accuracy', plot=plt)\n",
    "\n",
    "        run.complete()\n",
    "    except Exception as e:\n",
    "        run.fail()\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(idx):\n",
    "    result = np.argmax(model.predict(x_test[idx:idx+1]))\n",
    "    label = np.argmax(y_test[idx])\n",
    "    print('Prediction: {} ({})'.format(result, label_dict[result]))\n",
    "    print('Label: {} ({})'.format(label, label_dict[label]))\n",
    "    #plt.imshow(1-x_test[idx][:, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for _ in range(1,10):\n",
    "    idx = random.randint(0, 47-1)\n",
    "    run_prediction(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_path = os.path.join(project_folder, \"keras\")\n",
    "os.makedirs(keras_path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(keras_path, \"model.json\"), 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "model.save_weights(os.path.join(keras_path, 'model.h5'))\n",
    "\n",
    "model.save(os.path.join(keras_path, 'full_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for root, dirs, files in os.walk(project_folder, topdown=False):\n",
    "    for filename in files:\n",
    "        source = os.path.join(root, filename)\n",
    "        name = source.replace(project_folder, \"./outputs\")\n",
    "        print(\"Uploading \" + filename)\n",
    "        run.upload_file(name, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
