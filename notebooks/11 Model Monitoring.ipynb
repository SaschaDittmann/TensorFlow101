{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enabling Data Collection for Models in Production\n",
    "With this notebook, you can learn how to collect input model data from your Azure Machine Learning service in an Azure Blob storage. Once enabled, this data collected gives you the opportunity:\n",
    "\n",
    "* Monitor data drifts as production data enters your model\n",
    "* Make better decisions on when to retrain or optimize your model\n",
    "* Retrain your model with the data collected\n",
    "\n",
    "## What data is collected?\n",
    "* Model input data (voice, images, and video are not supported) from services deployed in Azure Kubernetes Cluster (AKS)\n",
    "* Model predictions using production input data.\n",
    "\n",
    "**Note:** pre-aggregation or pre-calculations on this data are done by user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import your dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Run\n",
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "from azureml.core.image import Image\n",
    "from azureml.core.model import Model\n",
    "\n",
    "import azureml.core\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "Initialize a workspace object from persisted configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(\"Resource group: \", ws.resource_group)\n",
    "print(\"Location: \", ws.location)\n",
    "print(\"Workspace name: \", ws.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project directory\n",
    "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script, and any additional files your training script depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = '../projects/model_monitoring'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model\n",
    "Register an existing trained model, add descirption and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.register(\n",
    "    model_path = \"./resources/models/sklearn_regression_model.pkl\", # this points to a local file\n",
    "    model_name = \"sklearn_regression_model\", # this is the name the model is registered as\n",
    "    tags = {'area': \"diabetes\", 'type': \"regression\"},\n",
    "    description = \"Ridge regression model to predict diabetes\",\n",
    "    workspace = ws\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Update your scoring file with Data Collection*\n",
    "### a. Import the module\n",
    "```python \n",
    "from azureml.monitoring import ModelDataCollector```\n",
    "### b. In your init function add:\n",
    "```python \n",
    "global inputs_dc, prediction_d\n",
    "inputs_dc = ModelDataCollector(\"best_model\", identifier=\"inputs\", feature_names=[\"feat1\", \"feat2\", \"feat3\", \"feat4\", \"feat5\", \"Feat6\"])\n",
    "prediction_dc = ModelDataCollector(\"best_model\", identifier=\"predictions\", feature_names=[\"prediction1\", \"prediction2\"])```\n",
    "    \n",
    "* Identifier: Identifier is later used for building the folder structure in your Blob, it can be used to divide \"raw\" data versus \"processed\".\n",
    "* CorrelationId: is an optional parameter, you do not need to set it up if your model doesn't require it. Having a correlationId in place does help you for easier mapping with other data. (Examples include: LoanNumber, CustomerId, etc.)\n",
    "* Feature Names: These need to be set up in the order of your features in order for them to have column names when the .csv is created.\n",
    "\n",
    "### c. In your run function add:\n",
    "```python\n",
    "inputs_dc.collect(data)\n",
    "prediction_dc.collect(result)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score_diabetes.py\n",
    "import pickle\n",
    "import json\n",
    "import numpy \n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import Ridge\n",
    "from azureml.core.model import Model\n",
    "\n",
    "from azureml.monitoring import ModelDataCollector\n",
    "import time\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    print (\"model initialized\" + time.strftime(\"%H:%M:%S\"))\n",
    "    # note here \"sklearn_regression_model.pkl\" is the name of the model registered under the workspace\n",
    "    # this call should return the path to the model.pkl file on the local disk.\n",
    "    model_path = Model.get_model_path(model_name = 'sklearn_regression_model')\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "    global inputs_dc, prediction_dc\n",
    "    # this setup will help us save our inputs under the \"inputs\" path in our Azure Blob\n",
    "    inputs_dc = ModelDataCollector(model_name=\"sklearn_regression_model\", identifier=\"inputs\", feature_names=[\"feat1\", \"feat2\"]) \n",
    "    # this setup will help us save our ipredictions under the \"predictions\" path in our Azure Blob\n",
    "    prediction_dc = ModelDataCollector(\"sklearn_regression_model\", identifier=\"predictions\", feature_names=[\"prediction1\", \"prediction2\"])\n",
    "\n",
    "def run(raw_data):\n",
    "    global inputs_dc, prediction_dc\n",
    "    try:\n",
    "        data = json.loads(raw_data)['data']\n",
    "        data = numpy.array(data)\n",
    "        print (\"saving input data\" + time.strftime(\"%H:%M:%S\"))\n",
    "        inputs_dc.collect(data) #this call is saving our input data into our blob\n",
    "        \n",
    "        result = model.predict(data)\n",
    "        print (\"saving prediction data\" + time.strftime(\"%H:%M:%S\"))\n",
    "        prediction_dc.collect(result)#this call is saving our prediction data into our blob\n",
    "        \n",
    "        # you can return any data type as long as it is JSON-serializable\n",
    "        return result.tolist()\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        print (error + time.strftime(\"%H:%M:%S\"))\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Update your myenv.yml file with the required module*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies.create(conda_packages=['numpy','scikit-learn'])\n",
    "myenv.add_pip_package(\"azureml-monitoring\")\n",
    "\n",
    "with open(os.path.join(project_folder, \"myenv.yml\"),\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your new Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.image import ContainerImage\n",
    "\n",
    "image_config = ContainerImage.image_configuration(\n",
    "    execution_script = \"score_diabetes.py\",\n",
    "    runtime = \"python\",\n",
    "    conda_file = os.path.join(project_folder, \"myenv.yml\"),\n",
    "    description = \"Image with ridge regression model\",\n",
    "    tags = {'area': \"diabetes\", 'type': \"regression\"}\n",
    ")\n",
    "\n",
    "image = ContainerImage.create(\n",
    "    name = \"diabetes-model\",\n",
    "    models = [model],\n",
    "    image_config = image_config,\n",
    "    workspace = ws\n",
    ")\n",
    "\n",
    "image.wait_for_creation(show_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.name, model.description, model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to AKS service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AKS compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "aks_name = 'myaks'\n",
    "\n",
    "try:\n",
    "    aks_target = AksCompute(workspace=ws, name=aks_name)\n",
    "    print('found existing:', aks_target.name)\n",
    "except ComputeTargetException:\n",
    "    print('creating new.')\n",
    "\n",
    "    # AKS configuration\n",
    "    prov_config = AksCompute.provisioning_configuration(\n",
    "        agent_count=3,\n",
    "        vm_size=\"Standard_B4ms\"\n",
    "    )\n",
    "    \n",
    "    # Create the cluster\n",
    "    aks_target = ComputeTarget.create(\n",
    "        workspace = ws, \n",
    "        name = aks_name, \n",
    "        provisioning_configuration = prov_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aks_target.wait_for_completion(show_output = True)\n",
    "print(aks_target.provisioning_state)\n",
    "print(aks_target.provisioning_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. *Activate Data Collection and App Insights through updating AKS Webservice configuration*\n",
    "In order to enable Data Collection and App Insights in your service you will need to update your AKS configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_config = AksWebservice.deploy_configuration(\n",
    "    collect_model_data=True, \n",
    "    enable_app_insights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Deploy your service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "aks_service_name ='diabetes-aks-svc'\n",
    "\n",
    "aks_service = Webservice.deploy_from_image(\n",
    "    workspace = ws, \n",
    "    name = aks_service_name,\n",
    "    image = image,\n",
    "    deployment_config = aks_config,\n",
    "    deployment_target = aks_target\n",
    ")\n",
    "\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your service and send some data\n",
    "**Note**: It will take around 15 mins for your data to appear in your blob.\n",
    "The data will appear in your Azure Blob following this format:\n",
    "\n",
    "/modeldata/subscriptionid/resourcegroupname/workspacename/webservicename/modelname/modelversion/identifier/year/month/day/data.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "test_sample = json.dumps({'data': [\n",
    "    [1,2,3,4,54,6,7,8,88,10], \n",
    "    [10,9,8,37,36,45,4,33,2,1]\n",
    "]})\n",
    "test_sample = bytes(test_sample,encoding = 'utf8')\n",
    "\n",
    "prediction = aks_service.run(input_data = test_sample)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "marthalc"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
